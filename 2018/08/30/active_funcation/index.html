
  <!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>激活函数 | 机器学习笔记</title>
  <meta name="author" content="LiuShuaiaCai">
  
  <meta name="description" content="Deep Learning">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="激活函数"/>
  <meta property="og:site_name" content="机器学习笔记"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="机器学习笔记" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>

  <body>
    <a name="top"></a>
  	<header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">机器学习笔记</a></h1>
  <h2><a href="/">Deep Learning</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about-me">About</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  	<div id="content" class="inner">
  	<div id="main-col" class="alignleft"><div id="wrapper"><article class="post hidden_show">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-08-30T06:06:08.000Z"><a href="/2018/08/30/active_funcation/">2018-08-30</a></time>
      
      
  
    <h1 class="title">激活函数</h1>
  

    </header>
    <div class="entry">
      
        <h2 id="1、激活函数介绍"><a href="#1、激活函数介绍" class="headerlink" title="1、激活函数介绍"></a>1、激活函数介绍</h2><h3 id="1-1、什么是激活函数"><a href="#1-1、什么是激活函数" class="headerlink" title="1.1、什么是激活函数"></a>1.1、什么是激活函数</h3><p>如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。<br><img src="/images/active.png" alt="activate"><br><a id="more"></a></p>
<h3 id="1-2、激活函数的特点"><a href="#1-2、激活函数的特点" class="headerlink" title="1.2、激活函数的特点"></a>1.2、激活函数的特点</h3><p>非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。<br>可微： 当优化方法是基于梯度的时候，这个性质是必须的。<br>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。<br>_ ： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效。<br>输出值范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的学习率。</p>
<h3 id="1-3、为什么要用激活函数"><a href="#1-3、为什么要用激活函数" class="headerlink" title="1.3、为什么要用激活函数"></a>1.3、为什么要用激活函数</h3><p>如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。<br>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p>
<h2 id="2、sigmoid函数"><a href="#2、sigmoid函数" class="headerlink" title="2、sigmoid函数"></a>2、sigmoid函数</h2><p>公式：<script type="math/tex">\sigma(x) = \frac{1}{1 + e^{-x}}</script></p>
<p><img src="/images/sigmoid.png" alt="sigmoid"><br>在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间内，这点很有意思，可以联想到概率，但是严格意义上讲，不要当成概率。sigmod函数曾经是比较流行的，它可以想象成一个神经元的放电率，在中间斜率比较大的地方是神经元的敏感区，在两边斜率很平缓的地方是神经元的抑制区。<br>当然，流行也是曾经流行，这说明函数本身是有一定的缺陷的。</p>
<h3 id="2-1、sigmoid函数作用"><a href="#2-1、sigmoid函数作用" class="headerlink" title="2.1、sigmoid函数作用"></a>2.1、sigmoid函数作用</h3><p>sigmoid函数也叫 Logistic 函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br>在特征相差比较复杂或是相差不是特别大时效果比较好。</p>
<h3 id="2-2、sigmoid缺点"><a href="#2-2、sigmoid缺点" class="headerlink" title="2.2、sigmoid缺点"></a>2.2、sigmoid缺点</h3><p>1) 当输入稍微远离了坐标原点，函数的梯度就变得很小了，几乎为零。在神经网络反向传播的过程中，我们都是通过微分的链式法则来计算各个权重w的微分的。当反向传播经过了sigmod函数，这个链条上的微分就很小很小了，况且还可能经过很多个sigmod函数，最后会导致权重w对损失函数几乎没影响，这样不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散。<br>2) 函数输出不是以0为中心的，这样会使权重更新效率降低。对于这个缺陷，在斯坦福的课程里面有详细的解释。<br>3) Sigmoids函数收敛缓慢；sigmod函数要进行指数运算，这个对于计算机来说是比较慢的。</p>
<h3 id="2-3、sigmoid函数求导"><a href="#2-3、sigmoid函数求导" class="headerlink" title="2.3、sigmoid函数求导"></a>2.3、sigmoid函数求导</h3><p>下面解释为何会出现梯度消失：<br>反向传播算法中，要对激活函数求导，sigmoid 的导数表达式为：<script type="math/tex">\Phi'(x) =  \Phi(x)(1-\Phi(x))</script><br>求导推理：<script type="math/tex">\sigma'(x) = (\frac {1}{1 + e^{-x}})'</script><br>=&gt; <script type="math/tex">= \frac {e^{-x}}{(1 + e^{-x})^2}</script><br>=&gt; <script type="math/tex">= \frac {1+e^{-x}-1}{(1+e^{-x})^2}</script><br>=&gt; <script type="math/tex">= \frac {1}{1+e^{-x}}(1-\frac {1}{1+e^{-x}})</script><br><br><br>=&gt; <script type="math/tex">= f(x)(1-f(x))</script></p>
<p>求导图像如下：<br><img src="/images/sigmoid_re.png" alt="sigmoid"><br>由图可知，导数从 0 开始很快就又趋近于 0 了，易造成“梯度消失”现象</p>
<p>python代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="comment"># sigmoid函数导数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">re_sigmoid</span><span class="params">(y)</span>:</span></span><br><span class="line">    y_ = y * (<span class="number">1</span>-y)</span><br><span class="line">    <span class="keyword">return</span> y_</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid 显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_sigmoid</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = sigmoid(x)</span><br><span class="line">    y_ = re_sigmoid(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    ax.plot(y, y_)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">2</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-0.5</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">plot_sigmoid()</span><br></pre></td></tr></table></figure></p>
<h2 id="3、正切函数（tanh函数）"><a href="#3、正切函数（tanh函数）" class="headerlink" title="3、正切函数（tanh函数）"></a>3、正切函数（tanh函数）</h2><p>公式：<script type="math/tex">\tanh(x) = \frac {\sinh(x)}{\cosh(x)} = \frac {e^x - e^{-x}}{e^x + e^{-x}}</script><br><img src="/images/tanh.png" alt="tanh"><br>tanh是双曲正切函数，tanh函数和sigmod函数的曲线是比较相近的，咱们来比较一下看看。首先相同的是，这两个函数在输入很大或是很小的时候，输出都几乎平滑，梯度很小，不利于权重更新；不同的是输出区间，tanh的输出区间是在(-1,1)之间，而且整个函数是以0为中心的，这个特点比sigmod的好。</p>
<p>一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。不过这些也都不是一成不变的，具体使用什么激活函数，还是要根据具体的问题来具体分析，还是要靠调试的。</p>
<p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tanh函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x=<span class="number">0</span>, bool=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> bool==<span class="keyword">False</span>:</span><br><span class="line">        y = (np.exp(x) - <span class="number">1</span>/np.exp(x)) / (np.exp(x) + <span class="number">1</span>/np.exp(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = np.tanh((<span class="number">0</span>, np.pi*<span class="number">1j</span>, np.pi*<span class="number">1j</span>/<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># tanh函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tanh</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = tanh(x)</span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-1.0</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">plot_tanh()</span><br></pre></td></tr></table></figure></p>
<h2 id="4、线性整流函数（ReLU函数）"><a href="#4、线性整流函数（ReLU函数）" class="headerlink" title="4、线性整流函数（ReLU函数）"></a>4、线性整流函数（ReLU函数）</h2><h3 id="4-1、定义（来自wiki）"><a href="#4-1、定义（来自wiki）" class="headerlink" title="4.1、定义（来自wiki）"></a>4.1、定义（来自<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">wiki</a>）</h3><p>线性整流函数（Rectified Linear Unit, ReLU）,又称修正线性单元, 是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。<br>通常意义下，线性整流函数指代数学中的斜坡函数，即</p>
<script type="math/tex; mode=display">f(x)=max(0,x)</script><p>而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换 <script type="math/tex">\mathbf {w} ^{T}\mathbf {x} +b</script>之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量 x，使用线性整流激活函数的神经元会输出</p>
<script type="math/tex; mode=display">\max(0,\mathbf {w} ^{T}\mathbf {x} +b)</script><p>至下一层神经元或作为整个神经网络的输出（取决现神经元在网络结构中所处位置）。<br><img src="/images/relu.png" alt="relu"></p>
<h3 id="4-2、优、缺点"><a href="#4-2、优、缺点" class="headerlink" title="4.2、优、缺点"></a>4.2、优、缺点</h3><p>优点：<br>1) 在输入为正数的时候，不存在梯度饱和问题，相比之下，逻辑函数在输入为0时达到 <script type="math/tex">\frac {1}{2}</script>，即已经是半饱和的稳定状态。<br>2) 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）<br>3）更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题</p>
<p>缺点：<br>1) 当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。<br>2) 我们发现ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</p>
<h3 id="4-3、ReLU的几种变形"><a href="#4-3、ReLU的几种变形" class="headerlink" title="4.3、ReLU的几种变形"></a>4.3、ReLU的几种变形</h3><p>4.3.1、带泄露线性整流（Leaky ReLU）<br>在输入值 x 为负的时候，带泄露线性整流函数（Leaky ReLU）的梯度为一个常数 <script type="math/tex">\lambda \in (0,1)</script>，而不是0。在输入值为正的时候，带泄露线性整流函数和普通斜坡函数保持一致。</p>
<script type="math/tex; mode=display">f(x)={\begin{cases}x&{\mbox{if }}x>0\\\lambda x&{\mbox{if }}x\leq 0\end{cases}}</script><p>4.3.2、参数线性整流（Parametric ReLU）<br>在深度学习中，如果设定 <script type="math/tex">\lambda</script>  为一个可通过<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">反向传播算法</a>（Backpropagation）学习的变量，那么带泄露线性整流又被称为参数线性整流（Parametric ReLU）</p>
<p>4.3.3、带泄露随机线性整流（Randomized Leaky ReLU, RReLU）<br>带泄露随机线性整流（Randomized Leaky ReLU, RReLU）最早是在Kaggle全美数据科学大赛（NDSB）中被首先提出并使用的。相比于普通带泄露线性整流函数，带泄露随机线性整流在负输入值段的函数梯度 <script type="math/tex">\lambda</script>  是一个取自连续性均匀分布 <script type="math/tex">U(l,u)</script>概率模型的随机变量，即</p>
<script type="math/tex; mode=display">f(x)={\begin{cases}x&{\mbox{if }}x>0\\\lambda x&{\mbox{if }}x\leq 0\end{cases}}</script><p>其中 <script type="math/tex">\lambda \sim U(l,u),l<u</script> 且 <script type="math/tex">l,u\in [0,1)</script>。</p>
<p>ReLU函数的python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReLU函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        y = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># ReLU函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_relu</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</span><br><span class="line">    y = list(map(relu,x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line"></span><br><span class="line">plot_relu()</span><br></pre></td></tr></table></figure></p>
<h2 id="5、softmax函数（wiki）"><a href="#5、softmax函数（wiki）" class="headerlink" title="5、softmax函数（wiki）"></a>5、softmax函数（<a href="https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">wiki</a>）</h2><p>在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量  <script type="math/tex">\mathbf {z}</script> “压缩”到另一个K维实向量  <script type="math/tex">\sigma (\mathbf {z} )</script> 中，使得每一个元素的范围都在 (0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：</p>
<script type="math/tex; mode=display">\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} j \in (1,k)</script><p>如图所示：<br><img src="/images/softmax.png" alt="softmax"></p>
<p>Softmax函数实际上是有限项离散概率分布的梯度对数归一化。因此，Softmax函数在包括 多项逻辑回归[1]:206–209 ，多项线性判别分析，朴素贝叶斯分类器和人工神经网络等的多种基于概率的多分类问题方法中都有着广泛应用。[2] 特别地，在多项逻辑回归和线性判别分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量 x 属于第 j 个分类的概率为：</p>
<script type="math/tex; mode=display">P(y=j|\mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}</script><p>这可以被视作K个线性函数 <script type="math/tex">\mathbf {x} \mapsto \mathbf {x} ^{\mathsf {T}}\mathbf {w} _{1},\ldots ,\mathbf {x} \mapsto \mathbf {x} ^{\mathsf {T}}\mathbf {w} _{K}</script> Softmax函数的复合（ <script type="math/tex">{\displaystyle \mathbf {x} ^{\mathsf {T}}\mathbf {w} } {\displaystyle \mathbf {x} }  {\displaystyle \mathbf {w} }</script> ）。</p>
<p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = np.exp(x)/sum(np.exp(x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_softmax</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    y = softmax(x)</span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-6</span>, <span class="number">6.1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-0.01</span>, <span class="number">0.12</span>, <span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">plot_softmax()</span><br></pre></td></tr></table></figure></p>
<h2 id="6、激活函数图"><a href="#6、激活函数图" class="headerlink" title="6、激活函数图"></a>6、激活函数图</h2><p><img src="/images/active_wiki.jpg" alt="active_wiki"></p>

      
    </div>


    <footer>
      
        

        
        <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina"></a>
    <a href="#" class="bds_tqq" data-cmd="tqq"></a>
    <a href="#" class="bds_renren" data-cmd="renren"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin"></a>
</div>

<script>
    window._bd_share_config = {
        "common": {
            "bdSnsKey": {},
            "bdText": "",
            "bdMini": "2",
            "bdPic": "",
            "bdStyle": "0",
            "bdSize": "16"
        },
        "share": {},
        "image": {
            "viewList": ["qzone", "tsina", "tqq", "renren", "weixin"],
            "viewText": "分享到",
            "viewSize": "16"
        },
        "selectShare": {
            "bdContainerClass": null,
            "bdSelectMiniList": ["qzone", "tsina", "tqq", "renren", "weixin"]
        }
    };
    with(document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];
</script>
      
      <div class="clearfix"></div>
    </footer>
  </div>
  <!-- 
    <section id="comment">
        <h1 class="title">留言</h1>
        <div class="ds-thread" data-thread-key="2018/08/30/active_funcation/"></div>
    </section>
 -->
</article>

  <aside id="sidebar" class="alignright">
         <!--02-->
        <div id="toc" class="toc-article">
    <div class="toc-title">目录</div>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、激活函数介绍"><span class="toc-text">1、激活函数介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1、什么是激活函数"><span class="toc-text">1.1、什么是激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2、激活函数的特点"><span class="toc-text">1.2、激活函数的特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3、为什么要用激活函数"><span class="toc-text">1.3、为什么要用激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、sigmoid函数"><span class="toc-text">2、sigmoid函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1、sigmoid函数作用"><span class="toc-text">2.1、sigmoid函数作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2、sigmoid缺点"><span class="toc-text">2.2、sigmoid缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3、sigmoid函数求导"><span class="toc-text">2.3、sigmoid函数求导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、正切函数（tanh函数）"><span class="toc-text">3、正切函数（tanh函数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、线性整流函数（ReLU函数）"><span class="toc-text">4、线性整流函数（ReLU函数）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1、定义（来自wiki）"><span class="toc-text">4.1、定义（来自wiki）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2、优、缺点"><span class="toc-text">4.2、优、缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3、ReLU的几种变形"><span class="toc-text">4.3、ReLU的几种变形</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、softmax函数（wiki）"><span class="toc-text">5、softmax函数（wiki）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、激活函数图"><span class="toc-text">6、激活函数图</span></a></li></ol>
</div>
      
  </aside>

<!-- <script src="/js/jquery-3.2.1.min.js"></script>
<script src="/js/syntax.js"></script> -->


</div></div>
      <div class="clearfix"></div>
  	</div>
    <div id = 'r_top'>
	<ul>
		<li class='top'><a href='#top'></a></li>
		<li class='close' data='0'><a href='javascript:;'></a></li>
		<li class='bottom'><a href='#bottom'></a></li>
	</ul>
</div>

    <footer id="footer" class="inner"><a name='bottom'></a>
<div class="alignleft">
  
  &copy; 2018 LiuShuaiaCai
  
</div>
<div class="clearfix"></div>
</footer>
    <script src="http://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js"></script>
<script type="text/javascript">
//<![CDATA[
if (typeof jQuery == 'undefined') {
  document.write(unescape("%3Cscript src='/js/jquery-2.0.3.min.js' type='text/javascript'%3E%3C/script%3E"));
}
// ]]>
</script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/javascript">
var duoshuo_shortname = 'feifan';
var duoshuoQuery = {short_name:duoshuo_shortname};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'https://unpkg.com/embed-js@5.0.3/umd/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>

<script src="/js/toc.js"></script>
<script src="/js/top.js"></script>
<script src="/js/syntax.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
  </html>

